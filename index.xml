<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Peter Izsak</title><link>/</link><description>Recent content on Peter Izsak</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 11 Mar 2019 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Publications</title><link>/publications/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>/publications/</guid><description>&lt;h4 id="2024">
 2024
 &lt;a class="heading-link" href="#2024">
 &lt;i class="fa fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h4>
&lt;p>&lt;strong>HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly&lt;/strong> &lt;br>
&lt;em>Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen&lt;/em> &lt;br>
Preprint &lt;br>
&lt;a href="https://arxiv.org/abs/2410.02694">paper&lt;/a> | &lt;a href="https://github.com/princeton-nlp/HELMET">Repo&lt;/a>&lt;/p>
&lt;p>&lt;strong>RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation&lt;/strong> &lt;br>
&lt;em>Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat and Peter Izsak&lt;/em> &lt;br>
Preprint &lt;br>
&lt;a href="https://arxiv.org/abs/2408.02545">Paper&lt;/a> | &lt;a href="https://github.com/IntelLabs/RAGFoundry">Repo&lt;/a>&lt;/p>
&lt;p>&lt;strong>CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity&lt;/strong> &lt;br>
&lt;em>Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, Peter Izsak&lt;/em> &lt;br>
The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024) Findings &lt;br>
&lt;a href="https://arxiv.org/abs/2404.10513">Paper&lt;/a>&lt;/p></description></item><item><title/><link>/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/about/</guid><description>&lt;p>I am a Research Scientist at Intel Labs, where I explore topics in the intersection of Deep Learning and Natural Language Processing. I completed my MSc and BSc at the Technion.&lt;/p>
&lt;p>I am highly interested and involved in working with large language models. In particular, efficiency aspects of all kinds of NLP models, hardware optimization aspects, extractive and generative Question Answering systems, few-shot capabilities of large language models and Information Retrieval systems.&lt;/p></description></item><item><title/><link>/projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/projects/</guid><description>&lt;p>TBD&lt;/p></description></item></channel></rss>