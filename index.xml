<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Peter Izsak</title><link>/</link><description>Recent content on Peter Izsak</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Publications</title><link>/publications/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>/publications/</guid><description>2022 Link to heading Transformer Language Models without Positional Encodings Still Learn Positional Information
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak and Omer Levy Preprint Paper
2021 Link to heading How to Train BERT with an Academic Budget
Peter Izsak, Moshe Berchansky and Omer Levy The 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021) Paper | Slides/Video | Code
2020 Link to heading Exploring the Boundaries of Low-Resource BERT Distillation</description></item><item><title/><link>/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/about/</guid><description>I am a Research Scientist at Intel Labs, where I explore topics in the intersection of Deep Learning and Natural Language Processing. I completed my MSc and BSc at the Technion.
I am highly interested and involved in working with large language models. In particular, efficiency aspects of all kinds of NLP models, hardware optimization aspects, extractive and generative Question Answering systems, few-shot capabilities of large language models and Information Retrieval systems.</description></item><item><title/><link>/projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/projects/</guid><description>TBD</description></item></channel></rss>