<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Peter Izsak</title><link>/</link><description>Recent content on Peter Izsak</description><generator>Hugo 0.125.1</generator><language>en</language><lastBuildDate>Mon, 11 Mar 2019 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Publications</title><link>/publications/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>/publications/</guid><description>2024 Link to heading CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, Peter Izsak Preprint Paper
2023 Link to heading Optimizing Retrieval-augmented Reader Models via Token Elimination Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan and Moshe Wasserblat The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) Paper | Code
2022 Link to heading Transformer Language Models without Positional Encodings Still Learn Positional Information</description></item><item><title/><link>/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/about/</guid><description>I am a Research Scientist at Intel Labs, where I explore topics in the intersection of Deep Learning and Natural Language Processing. I completed my MSc and BSc at the Technion.
I am highly interested and involved in working with large language models. In particular, efficiency aspects of all kinds of NLP models, hardware optimization aspects, extractive and generative Question Answering systems, few-shot capabilities of large language models and Information Retrieval systems.</description></item><item><title/><link>/projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/projects/</guid><description>TBD</description></item></channel></rss>