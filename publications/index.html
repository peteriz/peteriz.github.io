<!doctype html><html lang=en><head><title>Publications · Peter Izsak</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Peter Izsak"><meta name=description content="Academic publications and research papers by Peter Izsak in Natural Language Processing, Machine Learning, and Information Retrieval."><meta name=keywords content="blog,developer,personal,research,deep learning,natural language processing,nlp"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Academic publications and research papers by Peter Izsak in Natural Language Processing, Machine Learning, and Information Retrieval."><meta property="og:url" content="/publications/"><meta property="og:site_name" content="Peter Izsak"><meta property="og:title" content="Publications"><meta property="og:description" content="Academic publications and research papers by Peter Izsak in Natural Language Processing, Machine Learning, and Information Retrieval."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:published_time" content="2019-03-11T00:00:00+00:00"><meta property="article:modified_time" content="2019-03-11T00:00:00+00:00"><link rel=canonical href=/publications/><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@600;700&family=Source+Sans+Pro:wght@300;400;500&display=swap" rel=stylesheet><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.d0aee113ddf9b39a335efdcaeb1d8a55e06654cd922ad02ffa0e54680f4b45d5.css integrity="sha256-0K7hE935s5ozXv3K6x2KVeBmVM2SKtAv+g5UaA9LRdU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/publications.min.bf992c807b8eceeb1e3b6280ab5dae2d62a588905a9cf4a92047af62e63703a9.css integrity="sha256-v5ksgHuOzuseO2KAq12uLWKliJBanPSpIEevYuY3A6k=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Peter Izsak
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/publications/>Publications</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=/publications/>Publications</a></h1></header><div class=year-section><h3 class=publication-year>2024</h3><div class=publication-entry><div class=publication-title>HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly</div><div class=publication-authors>Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
International Conference on Learning Representations (ICLR 2025)</div><div class=publication-links><a href=https://arxiv.org/abs/2410.02694 class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper
</a><span class=link-separator>|</span>
<a href="https://openreview.net/forum?id=293V3bJbmE" class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> OpenReview
</a><span class=link-separator>|</span>
<a href=https://github.com/princeton-nlp/HELMET class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Repo</a></div></div><div class=publication-entry><div class=publication-title>RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</div><div class=publication-authors>Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat and Peter Izsak</div><div class=publication-venue><span class="publication-badge preprint">Preprint</span>
Preprint</div><div class=publication-links><a href=https://arxiv.org/abs/2408.02545 class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper
</a><span class=link-separator>|</span>
<a href=https://github.com/IntelLabs/RAGFoundry class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Repo</a></div></div><div class=publication-entry><div class=publication-title>CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity</div><div class=publication-authors>Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, Peter Izsak</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024) Findings</div><div class=publication-links><a href=https://arxiv.org/abs/2404.10513 class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper</a></div></div></div><div class=year-section><h3 class=publication-year>2023</h3><div class=publication-entry><div class=publication-title>Optimizing Retrieval-augmented Reader Models via Token Elimination</div><div class=publication-authors>Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan and Moshe Wasserblat</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)</div><div class=publication-links><a href=https://arxiv.org/abs/2310.13682 class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper</a></div></div></div><div class=year-section><h3 class=publication-year>2022</h3><div class=publication-entry><div class=publication-title>Transformer Language Models without Positional Encodings Still Learn Positional Information</div><div class=publication-authors>Adi Haviv, Ori Ram, Ofir Press, Peter Izsak and Omer Levy</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
Findings of the Association for Computational Linguistics: EMNLP 2022</div><div class=publication-links><a href=https://aclanthology.org/2022.findings-emnlp.99/ class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper</a></div></div></div><div class=year-section><h3 class=publication-year>2021</h3><div class=publication-entry><div class=publication-title>How to Train BERT with an Academic Budget</div><div class=publication-authors>Peter Izsak, Moshe Berchansky and Omer Levy</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
The 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021)</div><div class=publication-links><a href=https://aclanthology.org/2021.emnlp-main.831.pdf class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper
</a><span class=link-separator>|</span>
<a href=https://underline.io/lecture/37795-how-to-train-bert-with-an-academic-budget class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Slides/Video
</a><span class=link-separator>|</span>
<a href=https://github.com/IntelLabs/academic-budget-bert class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Repo</a></div></div></div><div class=year-section><h3 class=publication-year>2020</h3><div class=publication-entry><div class=publication-title>Exploring the Boundaries of Low-Resource BERT Distillation</div><div class=publication-authors>Moshe Wasserblat, Oren Pereg and Peter Izsak</div><div class=publication-venue><span class="publication-badge workshop">Workshop</span>
Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, 2020</div><div class=publication-links><a href=https://aclanthology.org/2020.sustainlp-1.5/ class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper</a></div></div></div><div class=year-section><h3 class=publication-year>2019</h3><div class=publication-entry><div class=publication-title>Training Compact Models for Low Resource Entity Tagging using Pre-trained Language Models</div><div class=publication-authors>Peter Izsak, Shira Guskin and Moshe Wasserblat</div><div class=publication-venue><span class="publication-badge workshop">Workshop</span>
2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS) (2019): 44-47.</div><div class=publication-links><a href=https://arxiv.org/pdf/1910.06294.pdf class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper
</a><span class=link-separator>|</span>
<a href=https://github.com/IntelLabs/nlp-architect class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Repo</a></div></div><div class=publication-entry><div class=publication-title>Q8BERT: Quantized 8Bit BERT</div><div class=publication-authors>Ofir Zafrir, Guy Boudoukh, Peter Izsak and Moshe Wasserblat</div><div class=publication-venue><span class="publication-badge workshop">Workshop</span>
2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS) (2019): 36-39.</div><div class=publication-links><a href=https://arxiv.org/pdf/1910.06188.pdf class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper
</a><span class=link-separator>|</span>
<a href=https://github.com/IntelLabs/nlp-architect class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Repo</a></div></div></div><div class=year-section><h3 class=publication-year>2018</h3><div class=publication-entry><div class=publication-title>Term Set Expansion based NLP Architect by Intel AI Lab</div><div class=publication-authors>Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Alon Eirew, Yael Green, Shira Guskin, Peter Izsak and Daniel Korat</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
The 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).</div><div class=publication-links><a href=https://aclanthology.org/D18-2004/ class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper
</a><span class=link-separator>|</span>
<a href=https://github.com/IntelLabs/nlp-architect class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Repo</a></div></div><div class=publication-entry><div class=publication-title>SetExpander: End-to-end Term Set Expansion Based on Multi-Context Term Embeddings</div><div class=publication-authors>Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Ido Dagan, Y. Goldberg, Alon Eirew, Yael Green, Shira Guskin, Peter Izsak and Daniel Korat</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations (COLING 2018)</div><div class=publication-links><a href=https://aclanthology.org/C18-2013/ class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper
</a><span class=link-separator>|</span>
<a href=https://github.com/IntelLabs/nlp-architect class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Repo</a></div></div></div><div class=year-section><h3 class=publication-year>2014</h3><div class=publication-entry><div class=publication-title>The search duel: a response to a strong ranker</div><div class=publication-authors>Peter Izsak, Fiana Raiber, Oren Kurland and Moshe Tennenholtz</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval (2014).</div><div class=publication-links><a href=https://ie.technion.ac.il/~kurland/searchDuel.pdf class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper</a></div></div></div><div class=year-section><h3 class=publication-year>2013</h3><div class=publication-entry><div class=publication-title>Leveraging memory mirroring for transparent memory scale-out with zero-downtime failover of remote hosts</div><div class=publication-authors>R. Tell, Peter Izsak, A. Shribman, Steve Walsh and B. Hudzia</div><div class=publication-venue><span class="publication-badge conference">Conference</span>
2013 IEEE Symposium on Computers and Communications (ISCC) (2013)</div><div class=publication-links><a href=https://ieeexplore.ieee.org/document/6754977 class=publication-link target=_blank rel=noopener><i class="fa fa-external-link" aria-hidden=true></i> Paper</a></div></div></div></article></section><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></div><footer class=footer><section class=container>©
2021 -
2026
Peter Izsak
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>