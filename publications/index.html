<!doctype html><html lang=en><head><title>Publications · Peter Izsak</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Peter Izsak"><meta name=description content="Sample article showcasing basic Markdown syntax and formatting for HTML elements."><meta name=keywords content="blog,developer,personal,research,deep learning,natural language processing,nlp"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Sample article showcasing basic Markdown syntax and formatting for HTML elements."><meta property="og:url" content="/publications/"><meta property="og:site_name" content="Peter Izsak"><meta property="og:title" content="Publications"><meta property="og:description" content="Sample article showcasing basic Markdown syntax and formatting for HTML elements."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:published_time" content="2019-03-11T00:00:00+00:00"><meta property="article:modified_time" content="2019-03-11T00:00:00+00:00"><link rel=canonical href=/publications/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.4b392a85107b91dbdabc528edf014a6ab1a30cd44cafcd5325c8efe796794fca.css integrity="sha256-SzkqhRB7kdvavFKO3wFKarGjDNRMr81TJcjv55Z5T8o=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Peter Izsak
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/publications/>Publications</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=/publications/>Publications</a></h1></header><h4 id=2024>2024
<a class=heading-link href=#2024><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly</strong><br><em>Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen</em><br>Preprint<br><a href=https://arxiv.org/abs/2410.02694 class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://github.com/princeton-nlp/HELMET class=external-link target=_blank rel=noopener>Repo</a></p><p><strong>RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</strong><br><em>Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat and Peter Izsak</em><br>Preprint<br><a href=https://arxiv.org/abs/2408.02545 class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://github.com/IntelLabs/RAGFoundry class=external-link target=_blank rel=noopener>Repo</a></p><p><strong>CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity</strong><br><em>Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, Peter Izsak</em><br>The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024) Findings<br><a href=https://arxiv.org/abs/2404.10513 class=external-link target=_blank rel=noopener>Paper</a></p><h4 id=2023>2023
<a class=heading-link href=#2023><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>Optimizing Retrieval-augmented Reader Models via Token Elimination</strong><br><em>Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan and Moshe Wasserblat</em><br>The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)<br><a href=https://arxiv.org/abs/2310.13682 class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://arxiv.org/abs/2310.13682 class=external-link target=_blank rel=noopener>Repo</a></p><h4 id=2022>2022
<a class=heading-link href=#2022><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>Transformer Language Models without Positional Encodings Still Learn Positional Information</strong><br><em>Adi Haviv, Ori Ram, Ofir Press, Peter Izsak and Omer Levy</em><br>Findings of the Association for Computational Linguistics: EMNLP 2022<br><a href=https://aclanthology.org/2022.findings-emnlp.99/ class=external-link target=_blank rel=noopener>Paper</a></p><h4 id=2021>2021
<a class=heading-link href=#2021><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>How to Train BERT with an Academic Budget</strong><br><em>Peter Izsak, Moshe Berchansky and Omer Levy</em><br>The 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021)<br><a href=https://aclanthology.org/2021.emnlp-main.831.pdf class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://underline.io/lecture/37795-how-to-train-bert-with-an-academic-budget class=external-link target=_blank rel=noopener>Slides/Video</a> | <a href=https://github.com/IntelLabs/academic-budget-bert class=external-link target=_blank rel=noopener>Repo</a></p><h4 id=2020>2020
<a class=heading-link href=#2020><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>Exploring the Boundaries of Low-Resource BERT Distillation</strong><br><em>Moshe Wasserblat, Oren Pereg and Peter Izsak</em><br>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, 2020<br><a href=https://aclanthology.org/2020.sustainlp-1.5/ class=external-link target=_blank rel=noopener>Paper</a></p><h4 id=2019>2019
<a class=heading-link href=#2019><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>Training Compact Models for Low Resource Entity Tagging using Pre-trained Language Models</strong><br><em>Peter Izsak, Shira Guskin and Moshe Wasserblat</em><br>2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS) (2019): 44-47.<br><a href=https://arxiv.org/pdf/1910.06294.pdf class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://github.com/IntelLabs/nlp-architect class=external-link target=_blank rel=noopener>Repo</a></p><p><strong>Q8BERT: Quantized 8Bit BERT</strong><br><em>Zafrir, Ofir, Guy Boudoukh, Peter Izsak and Moshe Wasserblat</em><br>2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS) (2019): 36-39.<br><a href=https://arxiv.org/pdf/1910.06188.pdf class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://github.com/IntelLabs/nlp-architect class=external-link target=_blank rel=noopener>Repo</a></p><h4 id=2018>2018
<a class=heading-link href=#2018><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>Term Set Expansion based NLP Architect by Intel AI Lab</strong><br><em>Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Alon Eirew, Yael Green, Shira Guskin, Peter Izsak and Daniel Korat</em><br>The 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).<br><a href=https://aclanthology.org/D18-2004/ class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://github.com/IntelLabs/nlp-architect class=external-link target=_blank rel=noopener>Repo</a></p><p><strong>SetExpander: End-to-end Term Set Expansion Based on Multi-Context Term Embeddings</strong><br><em>Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Ido Dagan, Y. Goldberg, Alon Eirew, Yael Green, Shira Guskin, Peter Izsak and Daniel Korat</em><br>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations (COLING 2018)<br><a href=https://aclanthology.org/C18-2013/ class=external-link target=_blank rel=noopener>Paper</a> | <a href=https://github.com/IntelLabs/nlp-architect class=external-link target=_blank rel=noopener>Repo</a></p><h4 id=2014>2014
<a class=heading-link href=#2014><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>The search duel: a response to a strong ranker</strong><br><em>Peter Izsak, Fiana Raiber, Oren Kurland and Moshe Tennenholtz</em><br>Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval (2014).<br><a href=https://ie.technion.ac.il/~kurland/searchDuel.pdf class=external-link target=_blank rel=noopener>Paper</a></p><h4 id=2013>2013
<a class=heading-link href=#2013><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><strong>Leveraging memory mirroring for transparent memory scale-out with zero-downtime failover of remote hosts</strong><br><em>Tell, R., Peter Izsak, A. Shribman, Steve Walsh and B. Hudzia</em><br>2013 IEEE Symposium on Computers and Communications (ISCC) (2013)<br><a href=https://ieeexplore.ieee.org/document/6754977 class=external-link target=_blank rel=noopener>Paper</a></p></article></section><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></div><footer class=footer><section class=container>©
2021 -
2026
Peter Izsak
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>